pldi2025:
  title: "Optimizing Ancilla-Based Quantum Circuits with SPARE"
  abstract: "<p>
    Many quantum algorithms instantiate and use ancillas, spare qubits that serve
    as temporary storage in a quantum circuit. In particular, many recently
    developed high-level and modular quantum programming languages (QPLs) use
    ancilla qubits to implement various programming constructs. These are lowered
    to circuits with nested/cascading compute-uncompute gate sequences that use
    ancilla qubits to track internal state. We present SPARE, a rewrite-based
    quantum circuit optimizer that restructures these compute-uncompute gate
    sequences, leveraging the ancilla qubit state information to optimize the
    circuit. In this work, we prove the correctness of SPARE’s rewrites and link
    SPARE’s gate-level transforms to language-level program rewrites, which may
    be performed on the input language. We evaluate SPARE on QPL-generated
    quantum circuits against Unqomp and Spire, two optimizing compilers for
    QPLs. SPARE achieves a reduction of up to 27.3% in qubit count, 56.7% in
    2-qubit gates, 68.2% in 1-qubit gates and 73.9% in depth against Unqomp, and
    up to 17.8% in qubits, 67.3% in 2-qubit gates, 61.4% in 1-qubit gates and
    59.9% in depth against Spire. We also evaluate SPARE against the Quartz,
    Feynman, and PyZX circuit optimizers: SPARE achieves up to a 70.0% reduction
    in two-qubit gates, up to a 53.6% reduction in 1-qubit gates, and up to a
    56.7% reduction in depth compared to the best result from all the gate-level
    optimizers.
    </p>"
  venue: "Conference on Programming Language Design and Implementation (PLDI)"
  year: 2025
  month: June
  pdf: /publications/pldi25-spare.pdf
  authors:
    - sharma
    - achour
  bibtex: "@article{10.1145/3729253,
    <br>  author = {Sharma, Ritvik and Achour, Sara},
    <br>  title = {Optimizing Ancilla-Based Quantum Circuits with SPARE},
    <br>  year = {2025},
    <br>  issue_date = {June 2025},
    <br>  publisher = {Association for Computing Machinery},
    <br>  address = {New York, NY, USA},
    <br>  volume = {9},
    <br>  number = {PLDI},
    <br>  url = {https://doi.org/10.1145/3729253},
    <br>  doi = {10.1145/3729253},
    <br>  abstract = {Many quantum algorithms instantiate and use ancillas, spare qubits
    <br>  that serve as temporary storage in a quantum circuit. In particular, many
    <br>  recently developed high-level and modular quantum programming languages (QPLs)
    <br>  use ancilla qubits to implement various programming constructs. These are
    <br>  lowered to circuits with nested/cascading compute-uncompute gate sequences that
    <br>  use ancilla qubits to track internal state. We present SPARE, a rewrite-based
    <br>  quantum circuit optimizer that restructures these compute-uncompute gate
    <br>  sequences, leveraging the ancilla qubit state information to optimize the
    <br>  circuit. In this work, we prove the correctness of SPARE’s rewrites and link
    <br>  SPARE’s gate-level transforms to language-level program rewrites, which may be
    <br>  performed on the input language. We evaluate SPARE on QPL-generated quantum
    <br>  circuits against Unqomp and Spire, two optimizing compilers for QPLs. SPARE
    <br>  achieves a reduction of up to 27.3% in qubit count, 56.7% in 2-qubit gates,
    <br>  68.2% in 1-qubit gates and 73.9% in depth against Unqomp, and up to 17.8% in
    <br>  qubits, 67.3% in 2-qubit gates, 61.4% in 1-qubit gates and 59.9% in depth
    <br>  against Spire. We also evaluate SPARE against the Quartz, Feynman, and PyZX
    <br>  circuit optimizers: SPARE achieves up to a 70.0% reduction in two-qubit gates,
    <br>  up to a 53.6% reduction in 1-qubit gates, and up to a 56.7% reduction in depth
    <br>  compared to the best result from all the gate-level optimizers.},
    <br>  journal = {Proc. ACM Program. Lang.},
    <br>  month = {jun},
    <br>  articleno = {154},
    <br>  numpages = {25},
    <br>  keywords = {quantum optimizing compilers, quantum programming languages}
    <br> }"

pldi2024-dare:
  title: "Compilation of Qubit Circuits to Optimized Qutrit Circuits"
  abstract: "<p>
    Quantum computers are a revolutionary class of computational platforms that
    are capable of solving computationally hard problems. However, today’s quantum
    hardware is subject to noise and decoherence issues that together limit the
    scale and complexity of the quantum circuits that can be implemented. Recently,
    practitioners have developed qutrit-based quantum hardware platforms that compute
    over |0⟩, |1⟩, and |2⟩ states, and have presented circuit depth reduction
    techniques using qutrits’ higher energy |2⟩ states to temporarily store
    information. However, thus far, such quantum circuits that use higher order
    states for temporary storage need to be manually crafted by hardware designers.
    We present Dare, an optimizing compiler for qutrit circuits that implement qubit
    computations. Dare deploys a qutrit circuit decomposition algorithm and a rewrite
    engine to construct and optimize qutrit circuits. We evaluate Dare against
    hand-optimized qutrit circuits and qubit circuits, and find Dare delivers up to
    65% depth improvement over manual qutrit implementations, and 43–75% depth
    improvement over qubit circuits. We also perform a fidelity analysis and find
    DARE-optimized qutrit circuits deliver up to 8.9× higher fidelity circuits than
    their manually implemented counterparts.
    </p>"
  venue: "Conference on Programming Language Design and Implementation (PLDI)"
  year: 2024
  month: June
  pdf: /publications/pldi24-qutrits.pdf
  authors:
    - sharma
    - achour
  bibtex: "@article{10.1145/3656388,
    <br>  author = {Sharma, Ritvik and Achour, Sara},
    <br>  title = {Compilation of Qubit Circuits to Optimized Qutrit Circuits},
    <br>  year = {2024},
    <br>  issue_date = {June 2024},
    <br>  publisher = {Association for Computing Machinery},
    <br>  address = {New York, NY, USA},
    <br>  volume = {8},
    <br>  number = {PLDI},
    <br>  url = {https://doi.org/10.1145/3656388},
    <br>  doi = {10.1145/3656388},
    <br>  abstract = {Quantum computers are a revolutionary class of computational
    <br>  platforms that are capable of solving computationally hard problems. However,
    <br>  today’s quantum hardware is subject to noise and decoherence issues that
    <br>  together limit the scale and complexity of the quantum circuits that can be
    <br>  implemented. Recently, practitioners have developed qutrit-based quantum
    <br>  hardware platforms that compute over |0⟩, |1⟩, and |2⟩ states, and have
    <br>  presented circuit depth reduction techniques using qutrits’ higher energy |2⟩
    <br>  states to temporarily store information. However, thus far, such quantum
    <br>  circuits that use higher order states for temporary storage need to be
    <br>  manually crafted by hardware designers. We present Dare, an optimizing
    <br>  compiler for qutrit circuits that implement qubit computations. Dare deploys a
    <br>  qutrit circuit decomposition algorithm and a rewrite engine to construct and
    <br>  optimize qutrit circuits. We evaluate Dare against hand-optimized qutrit
    <br>  circuits and qubit circuits, and find Dare delivers up to 65% depth
    <br>  improvement over manual qutrit implementations, and 43–75% depth improvement
    <br>  over qubit circuits. We also perform a fidelity analysis and find
    <br>  DARE-optimized qutrit circuits deliver up to 8.9\\texttimes{} higher fidelity
    <br>  circuits than their manually implemented counterparts.},
    <br>  journal = {Proc. ACM Program. Lang.},
    <br>  month = {jun},
    <br>  articleno = {158},
    <br>  numpages = {24},
    <br>  keywords = {Quantum computing, Qutrits, Synthesis, Rewriting Tools}
    <br> }"


hotchips2024:
  title: "Onyx: A Programmable Accelerator for Sparse Tensor Algebra"
  year: 2024
  month: August
  venue: IEEE Hot Chips Symposium (Hot Chips)
  authors: 
    - koul
    - strange
    - melchert
    - carsello
    - mei
    - hsu
    - kong
    - chen
    - ke
    - zhangkeyi
    - liu
    - nyengele
    - balasingam
    - adivarahan 
    - sharma
    - xie
    - torng
    - emer
    - kjolstad
    - horowitz
    - raina





vlsi2024:
  title: "Onyx: A 12nm 756 GOPS/W Coarse-Grained Reconfigurable Array for Accelerating Dense and Sparse Applications" 
  venue: "IEEE Symposium on VLSI Technology & Circuits (VLSI)"
  abstract: "<p>
    Onyx is the first fully programmable accelerator for
    arbitrary sparse tensor algebra kernels. Unlike prior work, it
    supports higher-order tensors, multiple inputs, and fusion. It
    achieves this with a coarse-grained reconfigurable array
    (CGRA) that has composable memory primitives for storing
    compressed any-order tensors and compute primitives that
    eliminate ineffectual computations in sparse expressions.
    Further, Onyx improves dense image processing and machine
    learning (ML) with application-specialized compute tiles,
    memory tiles optimized for affine access patterns, and hybrid
    clock gating in the global buffer. We achieve up to 565x better
    energy-delay product (EDP) for sparse kernels vs. CPUs with
    sparse libraries, and up to 76% and 85% lower EDP for image
    processing and ML, respectively, vs. Amber [1].
    </p>"
  year: 2024
  month: June
  url: "https://ieeexplore.ieee.org/document/10631383"
  articlenote: The above PDF is the author-submitted version of the article. The final published version can be found at the Article URL above.
  authors: 
    - koul
    - strange
    - melchert
    - carsello
    - mei
    - hsu
    - kong
    - chen
    - ke
    - zhangkeyi
    - liu
    - nyengele
    - balasingam
    - adivarahan 
    - sharma
    - xie
    - torng
    - emer
    - kjolstad
    - horowitz
    - raina
  bibtex: "@INPROCEEDINGS{10631383,
    <br>  author={Koul, Kalhan and Strange, Maxwell and Melchert, Jackson and Carsello,
    <br>  Alex and Mei, Yuchen and Hsu, Olivia and Kong, Taeyoung and Chen, Po-Han and
    <br>  Ke, Huifeng and Zhang, Keyi and Liu, Qiaoyi and Nyengele, Gedeon and
    <br>  Balasingam, Akhilesh and Adivarahan, Jayashree and Sharma, Ritvik and Xie,
    <br>  Zhouhua and Torng, Christopher and Emer, Joel and Kjolstad, Fredrik and
    <br>  Horowitz, Mark and Raina, Priyanka},
    <br>  booktitle={2024 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits)}, 
    <br>  title={Onyx: A 12nm 756 GOPS/W Coarse-Grained Reconfigurable Array for Accelerating Dense and Sparse Applications}, 
    <br>  year={2024},
    <br>  volume={},
    <br>  number={},
    <br>  pages={1-2},
    <br>  keywords={Tensors;Image coding;Algebra;Machine learning;Very large scale integration;Libraries;Kernel},
    <br>  doi={10.1109/VLSITechnologyandCir46783.2024.10631383}
    <br>  }"

asplos2023: 
  title: The Sparse Abstract Machine
  abstract: "<p>
    We propose the Sparse Abstract Machine (SAM), an abstract machine model for
    targeting sparse tensor algebra to reconfigurable and fixed-function spatial
    dataflow accelerators. SAM defines a streaming dataflow abstraction with sparse
    primitives that encompass a large space of scheduled tensor algebra
    expressions. SAM dataflow graphs naturally separate tensor formats from
    algorithms and are expressive enough to incorporate arbitrary iteration
    orderings and many hardware-specific optimizations. We also present Custard, a
    compiler from a high-level language to SAM that demonstrates SAM's usefulness
    as an intermediate representation. We automatically bind from SAM to a
    streaming dataflow simulator. We evaluate the generality and extensibility of
    SAM, explore the performance space of sparse tensor algebra optimizations using
    SAM, and show SAM's ability to represent dataflow hardware.
</p>"
  venue: "International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)"
  year: 2023
  pdf: /publications/asplos2022-sam.pdf
  month: March
  authors: 
    - hsu
    - strange
    - sharma
    - won
    - olukotun
    - emer
    - horowitz
    - kjolstad
  bibtex: "@inproceedings{hsu2023asplos,
    <br>    author = {Hsu, Olivia and Strange, Maxwell and Sharma, Ritvik and
    <br>    Won, Jaeyeon and Olukotun, Kunle and Emer, Joel S. and Horowitz, Mark A. and
    <br>    Kj\\o{}lstad, Fredrik},
    <br>    title = {The Sparse Abstract Machine},
    <br>    year = {2023},
    <br>    isbn = {9781450399180},
    <br>    publisher = {Association for Computing Machinery},
    <br>    address = {New York, NY, USA},
    <br>    url = {https://doi.org/10.1145/3582016.3582051},
    <br>    doi = {10.1145/3582016.3582051},
    <br>    abstract = {We propose the Sparse Abstract Machine (SAM), an abstract machine
    <br>    model for targeting sparse tensor algebra to reconfigurable and fixed-function
    <br>    spatial dataflow accelerators. SAM defines a streaming dataflow abstraction
    <br>    with sparse primitives that encompass a large space of scheduled tensor algebra
    <br>    expressions. SAM dataflow graphs naturally separate tensor formats from
    <br>    algorithms and are expressive enough to incorporate arbitrary iteration
    <br>    orderings and many hardware-specific optimizations. We also present Custard, a
    <br>    compiler from a high-level language to SAM that demonstrates SAM's usefulness
    <br>    as an intermediate representation. We automatically bind from SAM to a
    <br>    streaming dataflow simulator. We evaluate the generality and extensibility of
    <br>    SAM, explore the performance space of sparse tensor algebra optimizations using
    <br>    SAM, and show SAM's ability to represent dataflow hardware.},
    <br>    booktitle = {Proceedings of the 28th ACM International Conference
    <br>    on Architectural Support for Programming Languages and Operating Systems,
    <br>    Volume 3},
    <br>    pages = {710–726},
    <br>    numpages = {17},
    <br>    keywords = {domain-specific, streams, abstract machine, sparse tensor algebra},
    <br>    location = {Vancouver, BC, Canada},
    <br>    series = {ASPLOS 2023}
    }"


asplos2023-apex:
  title: "APEX: A Framework for Automated Processing Element Design Space Exploration using Frequent Subgraph Analysis"
  abstract: "<p>
    The architecture of a coarse-grained reconfigurable array (CGRA) processing
    element (PE) has a significant effect on the performance and energy-efficiency
    of an application running on the CGRA. This paper presents APEX, an automated
    approach for generating specialized PE architectures for an application or an
    application domain. APEX first analyzes application domain benchmarks using
    frequent subgraph mining to extract commonly occurring computational subgraphs.
    APEX then generates specialized PEs by merging subgraphs using a datapath graph
    merging algorithm. The merged datapath graphs are translated into a PE
    specification from which we automatically generate the PE hardware description
    in Verilog along with a compiler that maps applications to the PE. The PE
    hardware and compiler are inserted into a flexible CGRA generation and
    compilation toolchain that allows for agile evaluation of CGRAs. We evaluate
    APEX for two domains, machine learning and image processing. For image
    processing applications, our automatically generated CGRAs with specialized PEs
    achieve from 5% to 30% less area and from 22% to 46% less energy compared to a
    general-purpose CGRA. For machine learning applications, our automatically
    generated CGRAs consume 16% to 59% less energy and 22% to 39% less area than a
    general-purpose CGRA. This work paves the way for creation of application
    domain-driven design-space exploration frameworks that automatically generate
    efficient programmable accelerators, with a much lower design effort for both
    hardware and compiler generation.
    </p>"
  venue: "Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)"
  year: 2023
  month: March
  pdf: /publications/asplos2022-apex.pdf
  authors:
    - melchert
    - feng
    - donovick
    - daly
    - sharma
    - barrett
    - horowitz
    - hanrahan
    - raina
  bibtex: "@inproceedings{10.1145/3582016.3582070,
    <br>  author = {Melchert, Jackson and Feng, Kathleen and Donovick, Caleb and Daly,
    <br>  Ross and Sharma, Ritvik and Barrett, Clark and Horowitz, Mark A. and Hanrahan,
    <br>  Pat and Raina, Priyanka},
    <br>  title = {APEX: A Framework for Automated Processing Element Design Space
    <br>  Exploration using Frequent Subgraph Analysis},
    <br>  year = {2023},
    <br>  isbn = {9781450399180},
    <br>  publisher = {Association for Computing Machinery},
    <br>  address = {New York, NY, USA},
    <br>  url = {https://doi.org/10.1145/3582016.3582070},
    <br>  doi = {10.1145/3582016.3582070},
    <br>  abstract = {The architecture of a coarse-grained reconfigurable array (CGRA)
    <br>  processing element (PE) has a significant effect on the performance and
    <br>  energy-efficiency of an application running on the CGRA. This paper presents
    <br>  APEX, an automated approach for generating specialized PE architectures for an
    <br>  application or an application domain. APEX first analyzes application domain
    <br>  benchmarks using frequent subgraph mining to extract commonly occurring
    <br>  computational subgraphs. APEX then generates specialized PEs by merging
    <br>  subgraphs using a datapath graph merging algorithm. The merged datapath graphs
    <br>  are translated into a PE specification from which we automatically generate
    <br>  the PE hardware description in Verilog along with a compiler that maps
    <br>  applications to the PE. The PE hardware and compiler are inserted into a
    <br>  flexible CGRA generation and compilation toolchain that allows for agile
    <br>  evaluation of CGRAs. We evaluate APEX for two domains, machine learning and
    <br>  image processing. For image processing applications, our automatically
    <br>  generated CGRAs with specialized PEs achieve from 5% to 30% less area and
    <br>  from 22% to 46% less energy compared to a general-purpose CGRA. For machine
    <br>  learning applications, our automatically generated CGRAs consume 16% to 59%
    <br>  less energy and 22% to 39% less area than a general-purpose CGRA. This work
    <br>  paves the way for creation of application domain-driven design-space
    <br>  exploration frameworks that automatically generate efficient programmable
    <br>  accelerators, with a much lower design effort for both hardware and compiler
    <br>  generation.},
    <br>  booktitle = {Proceedings of the 28th ACM International Conference on
    <br>  Architectural Support for Programming Languages and Operating Systems,
    <br>  Volume 3},
    <br>  pages = {33--45},
    <br>  numpages = {13},
    <br>  keywords = {subgraph, reconfigurable accelerators, processing elements,
    <br>  hardware-software co-design, graph analysis, domain-specific accelerators,
    <br>  design space exploration, CGRA},
    <br>  location = {Vancouver, BC, Canada},
    <br>  series = {ASPLOS 2023}
    <br> }"



tecs2023-codebench:
  title: "CODEBench: A Neural Architecture and Hardware Accelerator Co-Design Framework"
  abstract: "<p>
    Recently, automated co-design of machine learning (ML) models and accelerator
    architectures has attracted significant attention from both the industry and
    academia. However, most co-design frameworks either explore a limited search
    space or employ suboptimal exploration techniques for simultaneous design
    decision investigations of the ML model and the accelerator. Furthermore,
    training the ML model and simulating the accelerator performance is
    computationally expensive. To address these limitations, this work proposes a
    novel neural architecture and hardware accelerator co-design framework, called
    CODEBench. It comprises two new benchmarking sub-frameworks, CNNBench and
    AccelBench, which explore expanded design spaces of convolutional neural
    networks (CNNs) and CNN accelerators. CNNBench leverages an advanced search
    technique, Bayesian Optimization using Second-order Gradients and Heteroscedastic
    Surrogate Model for Neural Architecture Search, to efficiently train a neural
    heteroscedastic surrogate model to converge to an optimal CNN architecture by
    employing second-order gradients. AccelBench performs cycle-accurate simulations
    for diverse accelerator architectures in a vast design space. With the proposed
    co-design method, our best CNN–accelerator pair achieves 1.4% higher accuracy on
    the CIFAR-10 dataset compared to the state-of-the-art pair while enabling 59.1%
    lower latency and 60.8% lower energy consumption. On the ImageNet dataset, it
    achieves 3.7% higher Top-1 accuracy at 43.8% lower latency and 11.2% lower
    energy consumption. CODEBench outperforms the state-of-the-art framework,
    Auto-NBA, by achieving 1.5% higher accuracy and 34.7× higher throughput while
    enabling 11.0× lower energy-delay product and 4.0× lower chip area on CIFAR-10.
    </p>"
  venue: "ACM Transactions on Embedded Computing Systems (TECS)"
  year: 2023
  month: April
  pdf: /publications/tcs22-codebench.pdf
  authors:
    - tuli
    - li
    - sharma
    - jha
  bibtex: "@article{10.1145/3575798,
    <br>  author = {Tuli, Shikhar and Li, Chia-Hao and Sharma, Ritvik and Jha, Niraj K.},
    <br>  title = {CODEBench: A Neural Architecture and Hardware Accelerator Co-Design Framework},
    <br>  year = {2023},
    <br>  issue_date = {May 2023},
    <br>  publisher = {Association for Computing Machinery},
    <br>  address = {New York, NY, USA},
    <br>  volume = {22},
    <br>  number = {3},
    <br>  issn = {1539-9087},
    <br>  url = {https://doi.org/10.1145/3575798},
    <br>  doi = {10.1145/3575798},
    <br>  abstract = {Recently, automated co-design of machine learning (ML) models and
    <br>  accelerator architectures has attracted significant attention from both the
    <br>  industry and academia. However, most co-design frameworks either explore a
    <br>  limited search space or employ suboptimal exploration techniques for
    <br>  simultaneous design decision investigations of the ML model and the
    <br>  accelerator. Furthermore, training the ML model and simulating the accelerator
    <br>  performance is computationally expensive. To address these limitations, this
    <br>  work proposes a novel neural architecture and hardware accelerator co-design
    <br>  framework, called CODEBench. It comprises two new benchmarking sub-frameworks,
    <br>  CNNBench and AccelBench, which explore expanded design spaces of convolutional
    <br>  neural networks (CNNs) and CNN accelerators. CNNBench leverages an advanced
    <br>  search technique, Bayesian Optimization using Second-order Gradients and
    <br>  Heteroscedastic Surrogate Model for Neural Architecture Search, to efficiently
    <br>  train a neural heteroscedastic surrogate model to converge to an optimal CNN
    <br>  architecture by employing second-order gradients. AccelBench performs
    <br>  cycle-accurate simulations for diverse accelerator architectures in a vast
    <br>  design space. With the proposed co-design method, our best CNN–accelerator
    <br>  pair achieves 1.4% higher accuracy on the CIFAR-10 dataset compared to the
    <br>  state-of-the-art pair while enabling 59.1% lower latency and 60.8% lower
    <br>  energy consumption. On the ImageNet dataset, it achieves 3.7% higher Top1
    <br>  accuracy at 43.8% lower latency and 11.2% lower energy consumption. CODEBench
    <br>  outperforms the state-of-the-art framework, i.e., Auto-NBA, by achieving 1.5%
    <br>  higher accuracy and 34.7\\texttimes{} higher throughput while enabling 11.0\\texttimes{}
    <br>  lower energy-delay product and 4.0\\texttimes{} lower chip area on CIFAR-10.},
    <br>  journal = {ACM Trans. Embed. Comput. Syst.},
    <br>  month = {apr},
    <br>  articleno = {51},
    <br>  numpages = {30},
    <br>  keywords = {Active learning, application-specific integrated circuits, hardware-software co-design, machine learning, Neural Architecture Search, neural network accelerators}
    <br> }"



iscas2021-crossbar:
  title: "A Crossbar Array of Analog-Digital-Hybrid Volatile Memory Synapse Cells for Energy-Efficient On-Chip Learning"
  abstract: "<p>
    This work presents a crossbar array of analog-digital-hybrid volatile memory
    synapse cells designed for energy-efficient on-chip learning. The proposed
    architecture leverages conventional silicon transistor-based synapse cells to
    achieve energy efficiency in neuromorphic computing systems. Using SPICE
    simulations, the design demonstrates potential advantages in terms of energy
    efficiency, integration, and scalability, enabling improved analog hardware
    neural networks and on-chip training for future system-on-chip designs.
    </p>"
  venue: "IEEE International Symposium on Circuits and Systems (ISCAS)"
  year: 2021
  month: May
  authors:
    - sharda
    - sharma
    - bhowmik
  bibtex: "@INPROCEEDINGS{9401530,
    <br>  author = {Sharda, Janak and Sharma, Ritvik and Bhowmik, Debanjan},
    <br>  booktitle = {2021 IEEE International Symposium on Circuits and Systems (ISCAS)},
    <br>  title = {A Crossbar Array of Analog-Digital-Hybrid Volatile Memory Synapse Cells
    <br>  for Energy-Efficient On-Chip Learning},
    <br>  year = {2021},
    <br>  pages = {1-5},
    <br>  keywords = {Nonvolatile memory; Neuromorphics; SPICE; Energy efficiency;
    <br>  System-on-chip; Transistors; Synapses; Neuromorphic Computing; Analog Hardware
    <br>  Neural Network; Conventional Silicon Transistor Synapse},
    <br>  doi = {10.1109/ISCAS51556.2021.9401530}
    <br> }"

iscas2020-adc:
  title: "A 4.2-pJ/Conv 10-b Asynchronous ADC with Hybrid Two-Tier Level-Crossing Event Coding"
  abstract: "<p>
    This paper presents a 10-bit asynchronous analog-to-digital converter (ADC)
    that achieves 4.2 pJ per conversion through a hybrid two-tier level-crossing
    event coding scheme. The proposed design leverages CMOS technology and novel
    event-driven encoding techniques to enable high-throughput continuous tracking
    with improved energy efficiency. Target applications include energy-constrained
    systems such as biomedical signal acquisition (e.g., electrocardiography) and
    other low-power computing platforms.
    </p>"
  venue: "IEEE International Symposium on Circuits and Systems (ISCAS)"
  year: 2020
  month: May
  authors:
    - kubendran
    - park
    - sharma
    - kim
    - joshi
    - cauwenberghs
    - ha
  bibtex: "@INPROCEEDINGS{9180458,
    <br>  author = {Kubendran, Rajkumar and Park, Jongkil and Sharma, Ritvik and Kim,
    <br>  Chul and Joshi, Siddharth and Cauwenberghs, Gert and Ha, Sohmyung},
    <br>  booktitle = {2020 IEEE International Symposium on Circuits and Systems (ISCAS)},
    <br>  title = {A 4.2-pJ/Conv 10-b Asynchronous ADC with Hybrid Two-Tier Level-Crossing
    <br>  Event Coding},
    <br>  year = {2020},
    <br>  pages = {1-5},
    <br>  keywords = {Capacitors; CMOS technology; Computer architecture;
    <br>  Electrocardiography; Clocks; Timing; Analog-to-digital conversion (ADC);
    <br>  asynchronous ADC; level-crossing ADC; high throughput continuous tracking},
    <br>  doi = {10.1109/ISCAS45731.2020.9180458}
    <br> }"
